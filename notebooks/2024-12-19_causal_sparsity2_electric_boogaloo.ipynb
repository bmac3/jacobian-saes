{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal sparsity 2: electric boogaloo\n",
    "We have nicely sparse Jacobians and that almost certainly implies sparsity in the causal graph.\n",
    "But what if my JSAEs are actually finding some sort of perverse solution that leads to sparse Jacobians but not sparse computation?\n",
    "\n",
    "For an illustration, imagine if the function of which we're taking the Jacobian is a high-degree polynomial, our SAEs could just be finding its local minima/maxima.\n",
    "This kind of thing seems _very_ unlikely to me considering that the function is affine-ReLUish-affine, but it's plausible.\n",
    "So we should check if there actually is causal sparsity using something other than the thing we're directly optimizing for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intervening on the input feature activations\n",
    "Very simple idea -- if our Jacobians say \"on this particular token, input feature X doesn't causally affect outpput feature Y\", then let's just try varying feature X (e.g. ablating it) and see if output feature Y changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m-deduped into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from jacobian_saes.utils import load_pretrained, default_prompt, run_sandwich\n",
    "\n",
    "layer = 3\n",
    "wandb_path = f\"lucyfarnik/jsaes_pythia70m1/sae_pythia-70m-deduped_blocks.{layer}.ln2.hook_normalized_16384:v0\"\n",
    "sae_pair, model, mlp_with_grads, _ = load_pretrained(wandb_path, device=\"cpu\")\n",
    "\n",
    "_, cache = model.run_with_cache(default_prompt, stop_at_layer=layer+1,\n",
    "                                names_filter=[sae_pair.cfg.hook_name])\n",
    "acts = cache[sae_pair.cfg.hook_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ablation_experiment(look_at_important_connections: bool = False, eps: float = 0.01):\n",
    "    \"\"\"\n",
    "    For each activation, use the Jacobian to find connections which are either\n",
    "    important or unimportant (depending on the value of look_at_important_connections).\n",
    "    Ablate the input feature in the connection and see how much the output feature changes.\n",
    "    \"\"\"\n",
    "    out_act_diffs = []\n",
    "    with tqdm(total=acts.shape[1]-1) as pbar:\n",
    "        for act in acts[0, 1:]:\n",
    "            jacobian, acts_dict = run_sandwich(sae_pair, mlp_with_grads, act)\n",
    "            sae_acts2 = acts_dict[\"sae_acts2\"]\n",
    "            topk_indices1 = acts_dict[\"topk_indices1\"]\n",
    "            topk_indices2 = acts_dict[\"topk_indices2\"]\n",
    "            if look_at_important_connections:\n",
    "                connections = (jacobian.abs() > eps).nonzero()\n",
    "            else:\n",
    "                connections = (jacobian.abs() < eps).nonzero()\n",
    "\n",
    "            for out_idx, in_idx in connections:\n",
    "                in_idx_in_d_sae = topk_indices1[in_idx]\n",
    "                out_idx_in_d_sae = topk_indices2[out_idx]\n",
    "                #! Make sure the indexing is aligned (i.e. that get_jacobian doesn't swap things around)\n",
    "                \n",
    "                # ablate in_idx and run through sandwich\n",
    "                act_abl = act - sae_pair.get_W_dec(False)[in_idx_in_d_sae] \n",
    "                mlp_out_abl, _ = mlp_with_grads(act_abl)\n",
    "                sae_acts2_abl = sae_pair.encode(mlp_out_abl, True)\n",
    "\n",
    "                # check that the values are similar\n",
    "                out_act_diffs.append((sae_acts2_abl[out_idx_in_d_sae] - sae_acts2[out_idx_in_d_sae]).item())\n",
    "\n",
    "\n",
    "            pbar.update(1)\n",
    "            diffs_tensor = torch.tensor(out_act_diffs)\n",
    "            pbar.set_description(f\"Mean diff: {diffs_tensor.mean():.3f} | Mean abs diff: {diffs_tensor.abs().mean():.3f} | N diffs: {len(out_act_diffs)}\")\n",
    "\n",
    "    return torch.tensor(out_act_diffs).abs().mean().item(), len(out_act_diffs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mean diff: -0.013 | Mean abs diff: 0.015 | N diffs: 778063: 100%|██████████| 858/858 [26:55<00:00,  1.88s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.014961636625230312, 778063)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_ablation_experiment(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mean diff: -0.082 | Mean abs diff: 0.099 | N diffs: 100529: 100%|██████████| 858/858 [04:22<00:00,  3.27it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0989634096622467, 100529)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_ablation_experiment(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
