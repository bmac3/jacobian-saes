{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's the fastest way to get the grads of the MLP activation function in TransformerLens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "import statistics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import timeit\n",
    "import transformer_lens\n",
    "from transformer_lens.components.mlps.mlp import MLP as TL_MLP\n",
    "from tqdm import tqdm\n",
    "from jaxtyping import Float\n",
    "from transformer_lens.utilities.addmm import batch_addmm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pythia models have parallel attn and MLP, for now let's just do the basic thing and use GPT2 small to avoid having to deal with that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tz20913/miniconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = transformer_lens.HookedTransformer.from_pretrained(\"gpt2-small\")\n",
    "model.cfg.parallel_attn_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 43, 768])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Somebody once told me the world is gonna roll me I ain't the sharpest tool in the shed She was looking kind of dumb with her finger and her thumb In the shape of an L on her forehead\"\n",
    "layer = 3\n",
    "_, cache = model.run_with_cache(prompt)\n",
    "resid_mid = cache['resid_mid', layer]\n",
    "resid_mid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPWithActGrads(\n",
       "  (hook_pre): HookPoint()\n",
       "  (hook_post): HookPoint()\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLPWithActGrads(TL_MLP):\n",
    "    def forward(\n",
    "        self, x: Float[torch.Tensor, \"batch pos d_model\"], return_act_grads: bool = False\n",
    "    ) -> Float[torch.Tensor, \"batch pos d_model\"]:\n",
    "        # This is equivalent to (roughly) W_in @ x + b_in. It's important to\n",
    "        # use a fused addmm to ensure it matches the Huggingface implementation\n",
    "        # exactly.\n",
    "        pre_act = self.hook_pre(batch_addmm(self.b_in, self.W_in, x))  # [batch, pos, d_mlp]\n",
    "\n",
    "        if (\n",
    "            self.cfg.is_layer_norm_activation()\n",
    "            and self.hook_mid is not None\n",
    "            and self.ln is not None\n",
    "        ):\n",
    "            raise NotImplementedError(\"You passed in something weird and I can't be bothered to support it rn, go check out the TransformerLens MLP code for what's supposed to go here and open a PR if you want this to work\")\n",
    "        else:\n",
    "            post_act = self.act_fn(pre_act) # [batch, pos, d_mlp]\n",
    "            if return_act_grads:\n",
    "                grad_of_act = torch.autograd.grad(\n",
    "                    outputs=post_act,\n",
    "                    inputs=pre_act,\n",
    "                    grad_outputs=torch.ones_like(post_act)\n",
    "                )[0]\n",
    "            post_act = self.hook_post(post_act)\n",
    "        output = batch_addmm(self.b_out, self.W_out, post_act)\n",
    "\n",
    "        if return_act_grads:\n",
    "            return output, grad_of_act\n",
    "        return output\n",
    "\n",
    "original_mlp = model.blocks[layer].mlp\n",
    "mlp_with_grads = MLPWithActGrads(original_mlp.cfg)\n",
    "mlp_with_grads.load_state_dict(original_mlp.state_dict())\n",
    "mlp_with_grads.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0004e+00,\n",
       "           1.0000e+00,  0.0000e+00],\n",
       "         [-1.9954e-02, -1.1823e-01,  6.7858e-01,  ...,  5.8370e-01,\n",
       "           7.6067e-01, -1.3031e-02],\n",
       "         [-2.9115e-06,  1.0245e+00, -4.3108e-02,  ..., -7.7937e-02,\n",
       "           2.0209e-01, -3.3486e-02],\n",
       "         ...,\n",
       "         [-4.3960e-02, -1.2179e-01, -1.3491e-02,  ...,  5.1352e-02,\n",
       "           7.8677e-01,  6.4266e-02],\n",
       "         [-5.5457e-02,  6.8279e-01,  0.0000e+00,  ...,  2.1305e-03,\n",
       "           5.6596e-01, -2.0028e-02],\n",
       "         [ 0.0000e+00,  8.5492e-01,  0.0000e+00,  ...,  5.9475e-01,\n",
       "           7.0861e-01, -3.8573e-02]]], device='mps:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_resid_post = original_mlp(resid_mid)\n",
    "my_resid_post, act_grads = mlp_with_grads(resid_mid, return_act_grads=True)\n",
    "assert torch.allclose(original_resid_post, my_resid_post)\n",
    "act_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
