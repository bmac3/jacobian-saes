{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "import statistics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import timeit\n",
    "import transformer_lens\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive testing (not proper benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAE(nn.Module):\n",
    "    def __init__(self, d_in: int, d_sae: int, k: int):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_sae = d_sae\n",
    "        self.k = k\n",
    "        self.W_enc = nn.Parameter(torch.randn(d_sae, d_in))\n",
    "        self.b_enc = nn.Parameter(torch.zeros(d_sae))\n",
    "        self.W_dec = nn.Parameter(torch.randn(d_in, d_sae))\n",
    "        self.b_dec = nn.Parameter(torch.zeros(d_in))\n",
    "\n",
    "    def encode(self, x: torch.Tensor):\n",
    "        acts = F.linear(x, self.W_enc, self.b_enc)\n",
    "        topk = torch.topk(acts, k=self.k, dim=-1)\n",
    "        return topk.values, topk.indices #? should a ReLU be here?\n",
    "\n",
    "    def decode(self, latents: torch.Tensor, indices: torch.Tensor):\n",
    "        # TODO it might be more efficient to instead pull the correct weights\n",
    "        # from W_dec rather than scattering, but idk how to make that work with batching\n",
    "        \n",
    "        x = torch.zeros(latents.size(0), self.d_sae, device=latents.device)\n",
    "        x.scatter_(1, indices, latents)\n",
    "        return F.linear(x, self.W_dec, self.b_dec)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x, indices = self.encode(x)\n",
    "        x = self.decode(x, indices)\n",
    "        return x, indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_resid: int, d_mlp: int):\n",
    "        super().__init__()\n",
    "        self.W_in = nn.Parameter(torch.randn(d_mlp, d_resid))\n",
    "        self.b_in = nn.Parameter(torch.zeros(d_mlp))\n",
    "        self.act = nn.ReLU()\n",
    "        self.W_out = nn.Parameter(torch.randn(d_resid, d_mlp))\n",
    "        self.b_out = nn.Parameter(torch.zeros(d_resid))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, return_grad_of_act: bool = False):\n",
    "        pre = F.linear(x, self.W_in, self.b_in)\n",
    "        if return_grad_of_act:\n",
    "            pre.requires_grad_(True)\n",
    "        post = self.act(pre)\n",
    "        out = F.linear(post, self.W_out, self.b_out)\n",
    "\n",
    "        if return_grad_of_act:\n",
    "            grad_of_act = torch.autograd.grad(\n",
    "                outputs=post,\n",
    "                inputs=pre,\n",
    "                grad_outputs=torch.ones_like(post)\n",
    "            )[0]\n",
    "            return out, grad_of_act\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_resid = 768\n",
    "d_mlp = 3072\n",
    "d_sae = 64 * d_resid\n",
    "k = 32\n",
    "n_tokens = 64\n",
    "\n",
    "sae = SAE(d_resid, d_sae, k).to(device) # operates both on resid_mid and resid_post\n",
    "mlp = MLP(d_resid, d_mlp).to(device)\n",
    "\n",
    "random_input = torch.randn(n_tokens, d_resid, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd1 = sae.W_dec.T @ mlp.W_in.T # (d_sae, d_mlp)\n",
    "w2e = mlp.W_out.T @ sae.W_enc.T # (d_mlp, d_sae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jacobian_slow(sae: SAE, mlp: MLP, resid_values: torch.Tensor):\n",
    "    # super lazy way of making sure these are on the same device\n",
    "    wd1 = sae.W_dec.T @ mlp.W_in.T # (d_sae, d_mlp)\n",
    "    w2e = mlp.W_out.T @ sae.W_enc.T # (d_mlp, d_sae)\n",
    "\n",
    "    sae_out, topk_indices1 = sae(resid_values) # (n_tokens, d_resid), (n_tokens, k)\n",
    "    resid_post, mlp_act_grad = mlp(sae_out, return_grad_of_act=True) # (n_token, d_resid), (n_token, d_mlp)\n",
    "    _, topk_indices2 = sae.encode(resid_post) # (n_tokens, k)\n",
    "\n",
    "    num_tokens = resid_values.size(0)\n",
    "    jacobian = torch.zeros(num_tokens, sae.k, sae.k)\n",
    "    for token_pos in range(resid_values.size(0)):\n",
    "        for output_k in range(sae.k):\n",
    "            output_k_sae_index = topk_indices2[token_pos, output_k]\n",
    "            for input_k in range(sae.k):\n",
    "                input_k_sae_index = topk_indices1[token_pos, input_k]\n",
    "                jacobian[token_pos, input_k, output_k] = (wd1[input_k_sae_index] * mlp_act_grad[token_pos] * w2e[:, output_k_sae_index]).sum()\n",
    "    \n",
    "    return jacobian\n",
    "\n",
    "jacobian1 = get_jacobian_slow(sae.cpu(), mlp.cpu(), random_input.cpu())\n",
    "jacobian1.shape\n",
    "sae = sae.to(device)\n",
    "mlp = mlp.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 32])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_jacobian_fast(sae: SAE, mlp: MLP, resid_values: torch.Tensor):\n",
    "    sae_out, topk_indices1 = sae(resid_values) # (n_tokens, d_resid), (n_tokens, k)\n",
    "    resid_post, mlp_act_grad = mlp(sae_out, return_grad_of_act=True) # (n_token, d_resid), (n_token, d_mlp)\n",
    "    _, topk_indices2 = sae.encode(resid_post) # (n_tokens, k)\n",
    "\n",
    "    return einops.einsum(wd1[topk_indices1], mlp_act_grad, w2e[:, topk_indices2],\n",
    "                         \"seq_pos k1 d_mlp, seq_pos d_mlp, d_mlp seq_pos k2 -> seq_pos k1 k2\")\n",
    "\n",
    "jacobian2 = get_jacobian_fast(sae, mlp, random_input)\n",
    "assert torch.allclose(jacobian1, jacobian2.cpu(), rtol=1e-5, atol=1e-1)\n",
    "jacobian2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For reference, here's how long a GPT forward pass takes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync():\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    elif device.type == \"mps\":\n",
    "        torch.mps.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tz20913/miniconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = transformer_lens.HookedTransformer.from_pretrained(\"gpt2-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6559516669949517\n",
      "0.1877600409789011\n",
      "0.17127637506928295\n",
      "0.16607412497978657\n",
      "0.17193058400880545\n",
      "0.1921600829809904\n",
      "0.17315387504640967\n",
      "0.16536600003018975\n",
      "0.1684253748971969\n",
      "0.16900570807047188\n"
     ]
    }
   ],
   "source": [
    "def fwd_pass():\n",
    "  model(\" Test\"*1024)\n",
    "  sync()\n",
    "\n",
    "for _ in range(10):\n",
    "  print(timeit.timeit(fwd_pass, number=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(55245) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Warming up: 100%|██████████| 10/10 [00:00<00:00, 22.93it/s]\n",
      "Benchmarking: 100%|██████████| 100/100 [00:01<00:00, 82.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_ms': 0.01084957709768787,\n",
       " 'median_ms': 0.010815750458277762,\n",
       " 'std_ms': 0.000570285220142084,\n",
       " 'min_ms': 0.009896458010189235,\n",
       " 'max_ms': 0.012661666958592832,\n",
       " 'device': 'mps',\n",
       " 'num_runs': 100}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_rand_inputs():\n",
    "    topk_indices1 = torch.randint(0, d_sae, (n_tokens, k), device=device)\n",
    "    mlp_act_grad = torch.randint(0, 2, (n_tokens, d_mlp), device=device)\n",
    "    topk_indices2 = torch.randint(0, d_sae, (n_tokens, k), device=device)\n",
    "    return topk_indices1, mlp_act_grad, topk_indices2\n",
    "\n",
    "def jacobian_einsum(topk_indices1, mlp_act_grad, topk_indices2):\n",
    "    jacobian = einops.einsum(wd1[topk_indices1], mlp_act_grad, w2e[:, topk_indices2],\n",
    "                             \"seq_pos k1 d_mlp, seq_pos d_mlp, d_mlp seq_pos k2 -> seq_pos k1 k2\")\n",
    "    sync()\n",
    "    return jacobian\n",
    "\n",
    "def timed_jacobian():\n",
    "    rand_inputs = get_rand_inputs()\n",
    "    sync()\n",
    "    start_time = time.perf_counter()\n",
    "    jacobian_einsum(*rand_inputs)\n",
    "    sync()\n",
    "    end_time = time.perf_counter()\n",
    "    return end_time - start_time\n",
    "\n",
    "# Warmup runs\n",
    "for _ in tqdm(range(10), desc=\"Warming up\"):\n",
    "    timed_jacobian()\n",
    "\n",
    "# Timing runs\n",
    "timings = []\n",
    "num_runs = 100\n",
    "for _ in tqdm(range(num_runs), desc=\"Benchmarking\"):\n",
    "    timings.append(timed_jacobian())\n",
    "\n",
    "# Calculate statistics\n",
    "stats = {\n",
    "    'mean_ms': statistics.mean(timings),\n",
    "    'median_ms': statistics.median(timings),\n",
    "    'std_ms': statistics.stdev(timings),\n",
    "    'min_ms': min(timings),\n",
    "    'max_ms': max(timings),\n",
    "    'device': str(device),\n",
    "    'num_runs': num_runs\n",
    "}\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
