{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do zeros in the Jacobian imply a lack of causal connections between variables?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "from jacobian_saes.utils import load_pretrained, default_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m-deduped into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "sae, model, mlp_with_grads, layer = load_pretrained(\"lucyfarnik/jsaes_pythia70m1/sae_pythia-70m-deduped_blocks.3.ln2.hook_normalized_16384:v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sandwich(mlp_in_features):\n",
    "    mlp_in_reconstr = sae.decode(mlp_in_features, False)\n",
    "    mlp_out, _ = mlp_with_grads(mlp_in_reconstr)\n",
    "    mlp_out_features = sae.encode(mlp_out, True)\n",
    "    return mlp_out_features\n",
    "\n",
    "def sliced_sandwich(mlp_in_features):\n",
    "    mlp_out_features = sandwich(mlp_in_features)\n",
    "    return mlp_out_features[mlp_out_features>0]\n",
    "\n",
    "def get_sliced_jac(mlp_in_features):\n",
    "    jacobian = torch.autograd.functional.jacobian(sliced_sandwich, mlp_in_features)\n",
    "    return jacobian[:, mlp_in_features>0]\n",
    "\n",
    "def get_sliced_jac2(mlp_in_features):\n",
    "    jacobian2 = torch.autograd.functional.jacobian(get_sliced_jac, mlp_in_features)\n",
    "    return jacobian2[:, :, mlp_in_features>0].diagonal(dim1=-2, dim2=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is there a noticeably correlation between having near-zero Jacobian values and near-zero Jacobian^2 values?\n",
    "By \"Jacobian^2\" I mean a matrix where $$J_{i,j}^2 = \\frac{d^2 y_i}{dx_j^2}$$\n",
    "\n",
    "This is kind of a weak signal, but it does give us some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(default_prompt, names_filter=[f\"blocks.{layer}.ln2.hook_normalized\"])\n",
    "mlp_in = cache[\"normalized\", layer, \"ln2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mlp_in' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m jacobians \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m jacobians2 \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m act \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[43mmlp_in\u001b[49m[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m      4\u001b[0m     mlp_in_features \u001b[38;5;241m=\u001b[39m sae\u001b[38;5;241m.\u001b[39mencode(act, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m     jacobians\u001b[38;5;241m.\u001b[39mappend(get_sliced_jac(mlp_in_features)\u001b[38;5;241m.\u001b[39mflatten())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mlp_in' is not defined"
     ]
    }
   ],
   "source": [
    "jacobians = []\n",
    "jacobians2 = []\n",
    "for act in tqdm(mlp_in[0]):\n",
    "    mlp_in_features = sae.encode(act, False)\n",
    "    jacobians.append(get_sliced_jac(mlp_in_features).flatten())\n",
    "    jacobians2.append(get_sliced_jac2(mlp_in_features).flatten())\n",
    "\n",
    "jacobians = torch.cat(jacobians)\n",
    "jacobians2 = torch.cat(jacobians2)\n",
    "jacobians.shape, jacobians.sum(), jacobians2.shape, jacobians2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cosine_similarity(jacobians, jacobians2, dim=0).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Jacobians over token positions\n",
    "We'd kinda expect connections to be sparse across input tokens.\n",
    "Specifically, we'd expect the degree of sparsity here to be such that roughly\n",
    "`n_avg_input_feats * n_feats` elements in the averaged Jacobian have absolute values\n",
    "substantially above 0 (in our case > 0.01). In the ideal scenario `n_avg_input_feats` \n",
    "should be around 5ish, so we'd expect about 0.03% of elements in the averaged Jacobian\n",
    "to be substantially non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d73eb8989654ddb8a1051a0d7e231ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2047it [04:17,  7.95it/s]0117%:   2%|▏         | 2047/100000 [04:18<3:10:04,  8.59it/s]\n",
      "1236it [02:27,  8.40it/s]0119%:   3%|▎         | 3283/100000 [06:46<3:04:52,  8.72it/s]\n",
      "81it [00:09,  8.25it/s]0.0118%:   3%|▎         | 3364/100000 [06:56<3:02:04,  8.85it/s]\n",
      "385it [00:43,  8.91it/s].0116%:   4%|▎         | 3749/100000 [07:40<2:57:46,  9.02it/s]\n",
      "515it [00:55,  9.36it/s].0112%:   4%|▍         | 4263/100000 [08:35<2:41:10,  9.90it/s] \n",
      "848it [01:55,  7.36it/s].0111%:   5%|▌         | 5112/100000 [10:31<3:10:05,  8.32it/s] \n",
      "229it [00:28,  8.13it/s].0110%:   5%|▌         | 5340/100000 [11:00<2:48:28,  9.36it/s]\n",
      "246it [00:28,  8.63it/s].0109%:   6%|▌         | 5587/100000 [11:29<2:43:00,  9.65it/s]\n",
      "300it [00:34,  8.68it/s].0109%:   6%|▌         | 5887/100000 [12:04<2:48:01,  9.33it/s]\n",
      "592it [01:14,  7.94it/s].0107%:   6%|▋         | 6479/100000 [13:19<3:35:19,  7.24it/s]\n",
      "77it [00:08,  9.24it/s]0.0106%:   7%|▋         | 6555/100000 [13:27<2:44:06,  9.49it/s]\n",
      "320it [00:41,  7.70it/s].0106%:   7%|▋         | 6876/100000 [14:09<4:00:11,  6.46it/s]\n",
      "718it [01:26,  8.32it/s].0104%:   8%|▊         | 7593/100000 [15:36<2:48:04,  9.16it/s] \n",
      "159it [00:18,  8.73it/s].0103%:   8%|▊         | 7753/100000 [15:55<2:38:15,  9.71it/s]\n",
      "166it [00:20,  8.29it/s].0103%:   8%|▊         | 7919/100000 [16:15<3:08:11,  8.16it/s]\n",
      "339it [00:39,  8.58it/s].0103%:   8%|▊         | 8258/100000 [16:55<3:21:23,  7.59it/s]\n",
      "1881it [03:35,  8.72it/s]0106%:  10%|█         | 10139/100000 [20:32<2:43:50,  9.14it/s]\n",
      "99it [00:11,  8.78it/s]0.0106%:  10%|█         | 10238/100000 [20:44<2:41:08,  9.28it/s] \n",
      "293it [00:31,  9.25it/s].0105%:  11%|█         | 10530/100000 [21:16<2:32:42,  9.76it/s]\n",
      "2047it [15:21,  2.22it/s]0106%:  13%|█▎        | 12577/100000 [36:38<2:31:20,  9.63it/s]   \n",
      "169it [00:17,  9.76it/s].0105%:  13%|█▎        | 12746/100000 [36:56<3:03:54,  7.91it/s]\n",
      "232it [00:24,  9.59it/s].0105%:  13%|█▎        | 12979/100000 [37:20<2:36:32,  9.26it/s]\n",
      "326it [00:35,  9.14it/s].0105%:  13%|█▎        | 13305/100000 [37:56<2:24:40,  9.99it/s]\n",
      "364it [00:41,  8.79it/s].0104%:  14%|█▎        | 13669/100000 [38:38<2:29:20,  9.64it/s]\n",
      "2047it [03:57,  8.62it/s]0107%:  16%|█▌        | 15715/100000 [42:36<2:28:17,  9.47it/s]\n",
      "182it [00:18,  9.66it/s].0107%:  16%|█▌        | 15898/100000 [42:55<2:31:59,  9.22it/s]\n",
      "191it [00:20,  9.36it/s].0106%:  16%|█▌        | 16089/100000 [43:16<2:17:46, 10.15it/s]\n",
      "796it [01:23,  9.49it/s].0105%:  17%|█▋        | 16884/100000 [44:40<2:17:37, 10.07it/s]\n",
      "123it [00:12,  9.88it/s].0105%:  17%|█▋        | 17008/100000 [44:53<2:22:40,  9.69it/s]\n",
      "330it [00:36,  9.16it/s].0104%:  17%|█▋        | 17338/100000 [45:29<2:20:42,  9.79it/s]\n",
      "566it [01:03,  8.90it/s].0104%:  18%|█▊        | 17904/100000 [46:33<2:19:12,  9.83it/s]\n",
      "1131it [02:08,  8.82it/s]0105%:  19%|█▉        | 19035/100000 [48:42<2:12:42, 10.17it/s]\n",
      "1672it [03:04,  9.07it/s]0105%:  21%|██        | 20707/100000 [51:47<3:07:04,  7.06it/s]\n",
      "2047it [04:05,  8.33it/s]0105%:  23%|██▎       | 22754/100000 [55:54<2:15:46,  9.48it/s] \n",
      "293it [00:35,  8.32it/s].0105%:  23%|██▎       | 23047/100000 [56:29<2:36:24,  8.20it/s] \n",
      "85it [00:10,  8.45it/s]0.0105%:  23%|██▎       | 23132/100000 [56:40<2:20:06,  9.14it/s]\n",
      "276it [00:30,  9.18it/s].0105%:  23%|██▎       | 23408/100000 [57:10<2:34:17,  8.27it/s]\n",
      "2047it [04:21,  7.82it/s]0104%:  25%|██▌       | 25455/100000 [1:01:32<2:14:56,  9.21it/s]\n",
      "222it [00:23,  9.53it/s].0104%:  26%|██▌       | 25677/100000 [1:01:56<2:14:35,  9.20it/s]\n",
      "2047it [04:07,  8.27it/s]0104%:  28%|██▊       | 27724/100000 [1:06:04<2:10:09,  9.26it/s] \n",
      "115it [00:12,  9.45it/s].0104%:  28%|██▊       | 27838/100000 [1:06:17<2:02:27,  9.82it/s]\n",
      "2047it [04:26,  7.69it/s]0103%:  30%|██▉       | 29886/100000 [1:10:44<2:02:31,  9.54it/s] \n",
      "1046it [01:58,  8.81it/s]0103%:  31%|███       | 30931/100000 [1:12:43<2:10:07,  8.85it/s]\n",
      "188it [00:20,  9.33it/s].0103%:  31%|███       | 31119/100000 [1:13:04<2:01:01,  9.49it/s]\n",
      "467it [00:51,  9.03it/s].0102%:  32%|███▏      | 31587/100000 [1:13:56<1:58:35,  9.62it/s]\n",
      "2047it [04:11,  8.13it/s]0102%:  34%|███▎      | 33634/100000 [1:18:09<2:05:08,  8.84it/s] \n",
      "753it [01:26,  8.74it/s].0101%:  34%|███▍      | 34387/100000 [1:19:36<1:57:23,  9.32it/s]\n",
      "2047it [04:14,  8.04it/s]0101%:  36%|███▋      | 36434/100000 [1:23:51<1:52:10,  9.44it/s] \n",
      "167it [00:19,  8.68it/s].0101%:  37%|███▋      | 36601/100000 [1:24:11<2:00:09,  8.79it/s]\n",
      "188it [00:21,  8.77it/s].0101%:  37%|███▋      | 36789/100000 [1:24:33<1:57:02,  9.00it/s]\n",
      "1127it [02:16,  8.27it/s]0100%:  38%|███▊      | 37916/100000 [1:26:49<2:13:28,  7.75it/s]\n",
      "59it [00:06,  9.38it/s]0.0100%:  38%|███▊      | 37974/100000 [1:26:56<1:42:41, 10.07it/s]\n",
      "130it [00:15,  8.34it/s].0100%:  38%|███▊      | 38105/100000 [1:27:12<1:49:28,  9.42it/s]\n",
      "2030it [04:06,  8.24it/s]0101%:  40%|████      | 40135/100000 [1:31:20<1:52:59,  8.83it/s] \n",
      "246it [01:32,  2.66it/s].0101%:  40%|████      | 40381/100000 [1:32:53<11:03:12,  1.50it/s]\n",
      "833it [09:51,  1.41it/s].0101%:  41%|████      | 41214/100000 [1:42:46<1:55:09,  8.51it/s]   \n",
      "382it [00:41,  9.25it/s].0101%:  42%|████▏     | 41596/100000 [1:43:27<1:55:35,  8.42it/s]\n",
      "193it [00:20,  9.41it/s].0100%:  42%|████▏     | 41788/100000 [1:43:48<1:39:54,  9.71it/s]\n",
      "358it [00:36,  9.81it/s].0100%:  42%|████▏     | 42146/100000 [1:44:25<1:35:26, 10.10it/s]\n",
      "729it [01:20,  9.08it/s].0100%:  43%|████▎     | 42875/100000 [1:45:45<1:33:57, 10.13it/s]\n",
      "248it [00:27,  8.89it/s].0100%:  43%|████▎     | 43124/100000 [1:46:14<2:00:26,  7.87it/s]\n",
      "563it [01:02,  9.06it/s].0100%:  44%|████▎     | 43687/100000 [1:47:16<1:53:45,  8.25it/s]\n",
      "353it [00:37,  9.46it/s].0100%:  44%|████▍     | 44039/100000 [1:47:54<1:39:04,  9.41it/s]\n",
      "1034it [01:57,  8.77it/s]0099%:  45%|████▌     | 45074/100000 [1:49:52<1:47:02,  8.55it/s]\n",
      "2047it [04:01,  8.49it/s]0100%:  47%|████▋     | 47121/100000 [1:53:55<1:36:47,  9.11it/s] \n",
      "373it [00:41,  9.05it/s].0100%:  47%|████▋     | 47493/100000 [1:54:36<1:40:41,  8.69it/s]\n",
      "108it [00:11,  9.57it/s].0100%:  48%|████▊     | 47602/100000 [1:54:48<1:31:37,  9.53it/s]\n",
      "699it [01:27,  7.95it/s].0100%:  48%|████▊     | 48301/100000 [1:56:17<1:38:50,  8.72it/s] \n",
      "Percentage above 0.01: 0.0100%:  48%|████▊     | 48301/100000 [1:56:17<2:04:28,  6.92it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     mlp_out_features \u001b[38;5;241m=\u001b[39m sandwich(mlp_in_features)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx2, (mlp_in_feats, mlp_out_feats) \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(mlp_in_features, mlp_out_features))):\n\u001b[0;32m---> 16\u001b[0m     jacobian \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43msliced_sandwich\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp_in_feats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     18\u001b[0m         full_jacobian_abs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(summed_abs_jacobians)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/functional.py:789\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    787\u001b[0m jac_i: Tuple[List[torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs)))  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(out\u001b[38;5;241m.\u001b[39mnelement()):\n\u001b[0;32m--> 789\u001b[0m     vj \u001b[38;5;241m=\u001b[39m \u001b[43m_autograd_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m el_idx, (jac_i_el, vj_el, inp_el) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[1;32m    797\u001b[0m         \u001b[38;5;28mzip\u001b[39m(jac_i, vj, inputs)\n\u001b[1;32m    798\u001b[0m     ):\n\u001b[1;32m    799\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m vj_el \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/functional.py:195\u001b[0m, in \u001b[0;36m_autograd_grad\u001b[0;34m(outputs, inputs, grad_outputs, create_graph, retain_graph, is_grads_batched)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m,) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnew_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnew_grad_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:496\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    492\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    493\u001b[0m         grad_outputs_\n\u001b[1;32m    494\u001b[0m     )\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    507\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    508\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    509\u001b[0m     ):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_tokens = 100_000\n",
    "dataset = load_dataset(\"monology/pile-uncopyrighted\", split=\"train\", streaming=True)\n",
    "\n",
    "summed_abs_jacobians = torch.zeros(sae.cfg.d_sae, sae.cfg.d_sae, device=sae.device)\n",
    "max_abs_jac_elements = torch.zeros_like(summed_abs_jacobians)\n",
    "num_tokens_processed = 0\n",
    "with tqdm(total=num_tokens) as pbar:\n",
    "    for idx, sample in enumerate(dataset):\n",
    "        with torch.no_grad():\n",
    "            _, cache = model.run_with_cache(sample[\"text\"], names_filter=[f\"blocks.{layer}.ln2.hook_normalized\"])\n",
    "            mlp_in = cache[\"normalized\", layer, \"ln2\"][0, 1:]\n",
    "            mlp_in_features = sae.encode(mlp_in, False)\n",
    "            mlp_out_features = sandwich(mlp_in_features)\n",
    "\n",
    "        for idx2, (mlp_in_feats, mlp_out_feats) in tqdm(enumerate(zip(mlp_in_features, mlp_out_features))):\n",
    "            jacobian = torch.autograd.functional.jacobian(sliced_sandwich, mlp_in_feats)\n",
    "            with torch.no_grad():\n",
    "                full_jacobian_abs = torch.zeros_like(summed_abs_jacobians)\n",
    "                full_jacobian_abs[mlp_out_feats>0] = jacobian.detach().abs()\n",
    "                summed_abs_jacobians += full_jacobian_abs\n",
    "                max_abs_jac_elements = torch.max(max_abs_jac_elements, full_jacobian_abs)\n",
    "                num_tokens_processed += 1\n",
    "                pbar.update(1)\n",
    "                if idx2 % 10 == 0:\n",
    "                    mean_abs_jacobian = summed_abs_jacobians / num_tokens_processed\n",
    "                    proportion_nonzero = (mean_abs_jacobian>0.01).float().mean()\n",
    "                    pbar.set_description(f\"Percentage above 0.01: {100*proportion_nonzero.item():.4f}%\")\n",
    "                if num_tokens_processed >= num_tokens:\n",
    "                    break\n",
    "\n",
    "        if num_tokens_processed >= num_tokens:\n",
    "            break\n",
    "\n",
    "mean_abs_jacobian = summed_abs_jacobians / num_tokens_processed\n",
    "(mean_abs_jacobian.abs()>0.01).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.9399% (1.4e+08 elements) are above 0e+00\n",
      "39.9037% (1.1e+08 elements) are above 1e-06\n",
      "13.6935% (3.7e+07 elements) are above 1e-05\n",
      "1.5901% (4.3e+06 elements) are above 1e-04\n",
      "0.2386% (6.4e+05 elements) are above 1e-03\n",
      "0.0340% (9.1e+04 elements) are above 5e-03\n",
      "0.0100% (2.7e+04 elements) are above 1e-02\n",
      "0.0000% (1.0e+01 elements) are above 1e-01\n",
      "0.0000% (0.0e+00 elements) are above 1e+00\n"
     ]
    }
   ],
   "source": [
    "for thresh in [0, 1e-6, 1e-5, 1e-4, 1e-3, 5e-3, 1e-2, 1e-1, 1]:\n",
    "    above_thresh = mean_abs_jacobian>thresh\n",
    "    print(f\"{100*above_thresh.float().mean().item():.4f}% ({above_thresh.sum().item():.1e} elements) are above {thresh:.0e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vary inputs, see how that changes the output variables where the partial derivative is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(default_prompt, names_filter=[f\"blocks.{layer}.ln2.hook_normalized\"])\n",
    "mlp_in = cache[\"normalized\", layer, \"ln2\"][0, 1:]\n",
    "mlp_in_features = sae.encode(mlp_in, False)\n",
    "mlp_out_features = sandwich(mlp_in_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for mlp_in_feats, mlp_out_feats in tqdm(zip(mlp_in_features, mlp_out_features)):\n",
    "    sliced_jac = get_sliced_jac(mlp_in_feats)\n",
    "    small_indices_in_sliced = (sliced_jac.abs() < 5e-3).nonzero()\n",
    "    \n",
    "    break #!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([443, 2])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_indices_in_sliced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  286,   683,   844,  1170,  1705,  1756,  2013,  3080,  4117,  5289,\n",
       "          5595,  5861,  6225,  8014,  8591,  8647,  9356,  9410,  9931, 10175,\n",
       "         10516, 10732, 10899, 12249, 12618, 13132, 13644, 13981, 14614, 15219,\n",
       "         15537, 16085], device='mps:0'),\n",
       " tensor([  307,   365,   479,   792,   936,  1302,  3338,  3507,  4307,  4366,\n",
       "          4463,  4485,  4560,  5150,  6380,  6394,  6712,  8943,  9023,  9692,\n",
       "          9714,  9717,  9982, 10270, 10608, 10756, 10976, 11524, 11781, 12469,\n",
       "         12590, 12671], device='mps:0'))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mlp_in_feats>0).nonzero().flatten(), (mlp_out_feats>0).nonzero().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
