{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do zeros in the Jacobian imply a lack of causal connections between variables?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "from jacobian_saes.utils import load_pretrained, default_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact sae_pythia-70m-deduped_blocks.3.ln2.hook_normalized_16384:v4, 136.15MB. 2 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   2 of 2 files downloaded.  \n",
      "Done. 0:0:0.8\n",
      "/Users/tz20913/Library/Mobile Documents/com~apple~CloudDocs/Desktop/Research/Jacobian SAEs/notebooks/../sae_lens/sae.py:148: UserWarning: \n",
      "This SAE has non-empty model_from_pretrained_kwargs. \n",
      "For optimal performance, load the model like so:\n",
      "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m-deduped into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "sae, model, mlp, layer = load_pretrained(\"lucyfarnik/jacobian_saes_test/sae_pythia-70m-deduped_blocks.3.ln2.hook_normalized_16384:v4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sandwich(mlp_in_features):\n",
    "    mlp_in_reconstr = sae.decode(mlp_in_features, False)\n",
    "    mlp_out = mlp(mlp_in_reconstr)\n",
    "    mlp_out_features = sae.encode(mlp_out, True)\n",
    "    return mlp_out_features\n",
    "\n",
    "def sliced_sandwich(mlp_in_features):\n",
    "    mlp_out_features = sandwich(mlp_in_features)\n",
    "    return mlp_out_features[mlp_out_features>0]\n",
    "\n",
    "def get_sliced_jac(mlp_in_features):\n",
    "    jacobian = torch.autograd.functional.jacobian(sliced_sandwich, mlp_in_features)\n",
    "    return jacobian[:, mlp_in_features>0]\n",
    "\n",
    "def get_sliced_jac2(mlp_in_features):\n",
    "    jacobian2 = torch.autograd.functional.jacobian(get_sliced_jac, mlp_in_features)\n",
    "    return jacobian2[:, :, mlp_in_features>0].diagonal(dim1=-2, dim2=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is there a noticeably correlation between having near-zero Jacobian values and near-zero Jacobian^2 values?\n",
    "By \"Jacobian^2\" I mean a matrix where $$J_{i,j}^2 = \\frac{d^2 y_i}{dx_j^2}$$\n",
    "\n",
    "This is kind of a weak signal, but it does give us some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(default_prompt, names_filter=[f\"blocks.{layer}.ln2.hook_normalized\"])\n",
    "mlp_in = cache[\"normalized\", layer, \"ln2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mlp_in' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m jacobians \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m jacobians2 \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m act \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[43mmlp_in\u001b[49m[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m      4\u001b[0m     mlp_in_features \u001b[38;5;241m=\u001b[39m sae\u001b[38;5;241m.\u001b[39mencode(act, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m     jacobians\u001b[38;5;241m.\u001b[39mappend(get_sliced_jac(mlp_in_features)\u001b[38;5;241m.\u001b[39mflatten())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mlp_in' is not defined"
     ]
    }
   ],
   "source": [
    "jacobians = []\n",
    "jacobians2 = []\n",
    "for act in tqdm(mlp_in[0]):\n",
    "    mlp_in_features = sae.encode(act, False)\n",
    "    jacobians.append(get_sliced_jac(mlp_in_features).flatten())\n",
    "    jacobians2.append(get_sliced_jac2(mlp_in_features).flatten())\n",
    "\n",
    "jacobians = torch.cat(jacobians)\n",
    "jacobians2 = torch.cat(jacobians2)\n",
    "jacobians.shape, jacobians.sum(), jacobians2.shape, jacobians2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cosine_similarity(jacobians, jacobians2, dim=0).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Jacobians over token positions\n",
    "There's a chance that the connections are sparse across the input distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ece20a80b540509ddd72e36dac14ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2047it [03:43,  9.17it/s]:   2%|▏         | 2046/100000 [03:57<2:42:48, 10.03it/s]\n",
      "1236it [02:15,  9.12it/s]:   3%|▎         | 3282/100000 [06:13<2:55:15,  9.20it/s]\n",
      "81it [00:08,  9.92it/s]1%:   3%|▎         | 3364/100000 [06:21<2:34:03, 10.45it/s]\n",
      "385it [00:41,  9.22it/s]%:   4%|▎         | 3749/100000 [07:03<4:05:36,  6.53it/s]\n",
      "515it [00:55,  9.34it/s]%:   4%|▍         | 4263/100000 [07:58<2:50:29,  9.36it/s]\n",
      "848it [01:36,  8.77it/s]%:   5%|▌         | 5112/100000 [09:35<2:35:57, 10.14it/s] \n",
      "229it [00:26,  8.78it/s]%:   5%|▌         | 5341/100000 [10:02<2:49:47,  9.29it/s]\n",
      "246it [00:26,  9.22it/s]%:   6%|▌         | 5587/100000 [10:29<2:36:35, 10.05it/s] \n",
      "300it [00:33,  9.02it/s]%:   6%|▌         | 5886/100000 [11:02<2:44:30,  9.53it/s]\n",
      "592it [01:04,  9.16it/s]%:   6%|▋         | 6479/100000 [12:07<3:07:04,  8.33it/s]\n",
      "77it [00:09,  8.48it/s]2%:   7%|▋         | 6555/100000 [12:16<2:54:57,  8.90it/s]\n",
      "320it [00:37,  8.42it/s]%:   7%|▋         | 6876/100000 [12:55<2:57:25,  8.75it/s]\n",
      "718it [01:28,  8.09it/s]%:   8%|▊         | 7594/100000 [14:24<2:48:45,  9.13it/s]\n",
      "159it [00:17,  8.90it/s]%:   8%|▊         | 7752/100000 [14:42<2:25:52, 10.54it/s]\n",
      "166it [00:19,  8.67it/s]%:   8%|▊         | 7919/100000 [15:01<2:42:25,  9.45it/s]\n",
      "339it [00:37,  9.12it/s]%:   8%|▊         | 8257/100000 [15:39<2:36:18,  9.78it/s]\n",
      "1881it [03:22,  9.29it/s]:  10%|█         | 10139/100000 [19:02<2:30:43,  9.94it/s]\n",
      "99it [00:10,  9.49it/s]0%:  10%|█         | 10238/100000 [19:13<2:28:54, 10.05it/s]\n",
      "293it [00:31,  9.38it/s]%:  11%|█         | 10530/100000 [19:44<2:28:57, 10.01it/s]\n",
      "2047it [03:26,  9.92it/s]:  13%|█▎        | 12578/100000 [23:11<2:18:33, 10.52it/s]\n",
      "169it [00:17,  9.86it/s]%:  13%|█▎        | 12747/100000 [23:28<2:20:13, 10.37it/s]\n",
      "232it [00:23,  9.96it/s]%:  13%|█▎        | 12979/100000 [23:51<2:26:54,  9.87it/s]\n",
      "326it [00:33,  9.71it/s]%:  13%|█▎        | 13304/100000 [24:25<2:26:18,  9.88it/s]\n",
      "364it [00:35, 10.11it/s]%:  14%|█▎        | 13669/100000 [25:01<2:21:44, 10.15it/s]\n",
      "2047it [03:20, 10.23it/s]:  16%|█▌        | 15715/100000 [28:22<2:13:37, 10.51it/s]\n",
      "182it [00:18,  9.81it/s]%:  16%|█▌        | 15898/100000 [28:40<2:28:05,  9.47it/s]\n",
      "191it [00:19,  9.97it/s]%:  16%|█▌        | 16089/100000 [29:00<2:14:17, 10.41it/s]\n",
      "796it [01:17, 10.26it/s]%:  17%|█▋        | 16885/100000 [30:18<2:13:09, 10.40it/s]\n",
      "123it [00:11, 10.30it/s]%:  17%|█▋        | 17008/100000 [30:30<2:20:59,  9.81it/s]\n",
      "330it [00:32, 10.19it/s]%:  17%|█▋        | 17338/100000 [31:03<2:08:53, 10.69it/s]\n",
      "566it [00:56,  9.94it/s]%:  18%|█▊        | 17903/100000 [32:00<2:37:38,  8.68it/s]\n",
      "1131it [01:58,  9.50it/s]:  19%|█▉        | 19035/100000 [33:59<2:07:16, 10.60it/s] \n",
      "1672it [02:44, 10.16it/s]:  21%|██        | 20707/100000 [36:44<2:14:38,  9.81it/s]\n",
      "2047it [03:22, 10.11it/s]:  23%|██▎       | 22753/100000 [40:07<2:11:35,  9.78it/s]\n",
      "293it [00:30,  9.68it/s]%:  23%|██▎       | 23047/100000 [40:38<2:09:49,  9.88it/s]\n",
      "85it [00:08, 10.18it/s]2%:  23%|██▎       | 23131/100000 [40:46<2:12:58,  9.63it/s]\n",
      "276it [00:30,  9.11it/s]%:  23%|██▎       | 23408/100000 [41:17<2:05:06, 10.20it/s]\n",
      "2047it [03:26,  9.89it/s]:  25%|██▌       | 25455/100000 [44:44<1:58:23, 10.49it/s]\n",
      "222it [00:23,  9.39it/s]%:  26%|██▌       | 25677/100000 [45:08<2:27:40,  8.39it/s]\n",
      "2047it [03:56,  8.66it/s]:  28%|██▊       | 27723/100000 [49:05<2:10:05,  9.26it/s]\n",
      "115it [00:13,  8.25it/s]%:  28%|██▊       | 27838/100000 [49:19<2:07:45,  9.41it/s]\n",
      "450it [00:50,  8.96it/s]%:  28%|██▊       | 28290/100000 [50:10<2:10:47,  9.14it/s]\n",
      "Nonzero percentage: 49.1%:  28%|██▊       | 28290/100000 [50:10<2:07:12,  9.40it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m     mean_abs_jacobian \u001b[38;5;241m=\u001b[39m summed_abs_jacobians \u001b[38;5;241m/\u001b[39m num_tokens_processed\n\u001b[1;32m     24\u001b[0m     proportion_nonzero \u001b[38;5;241m=\u001b[39m (mean_abs_jacobian\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m---> 25\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNonzero percentage: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43mproportion_nonzero\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_tokens_processed \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m num_tokens:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_tokens = 100_000\n",
    "dataset = load_dataset(\"monology/pile-uncopyrighted\", split=\"train\", streaming=True)\n",
    "\n",
    "summed_abs_jacobians = torch.zeros(sae.cfg.d_sae, sae.cfg.d_sae, device=sae.device)\n",
    "max_abs_jac_elements = torch.zeros_like(summed_abs_jacobians)\n",
    "num_tokens_processed = 0\n",
    "with tqdm(total=num_tokens) as pbar:\n",
    "    for idx, sample in enumerate(dataset):\n",
    "        with torch.no_grad():\n",
    "            _, cache = model.run_with_cache(sample[\"text\"], names_filter=[f\"blocks.{layer}.ln2.hook_normalized\"])\n",
    "            mlp_in = cache[\"normalized\", layer, \"ln2\"][0, 1:]\n",
    "            mlp_in_features = sae.encode(mlp_in, False)\n",
    "            mlp_out_features = sandwich(mlp_in_features)\n",
    "\n",
    "        for idx2, (mlp_in_feats, mlp_out_feats) in tqdm(enumerate(zip(mlp_in_features, mlp_out_features))):\n",
    "            jacobian = torch.autograd.functional.jacobian(sliced_sandwich, mlp_in_feats)\n",
    "            with torch.no_grad():\n",
    "                full_jacobian_abs = torch.zeros_like(summed_abs_jacobians)\n",
    "                full_jacobian_abs[mlp_out_feats>0] = jacobian.detach().abs()\n",
    "                summed_abs_jacobians += full_jacobian_abs\n",
    "                max_abs_jac_elements = torch.max(max_abs_jac_elements, full_jacobian_abs)\n",
    "                num_tokens_processed += 1\n",
    "                pbar.update(1)\n",
    "                if idx2 % 10 == 0:\n",
    "                    mean_abs_jacobian = summed_abs_jacobians / num_tokens_processed\n",
    "                    proportion_nonzero = (mean_abs_jacobian>0).float().mean()\n",
    "                    pbar.set_description(f\"Nonzero percentage: {100*proportion_nonzero.item():.1f}%\")\n",
    "                if num_tokens_processed >= num_tokens:\n",
    "                    break\n",
    "\n",
    "        if num_tokens_processed >= num_tokens:\n",
    "            break\n",
    "\n",
    "mean_abs_jacobian = summed_abs_jacobians / num_tokens_processed\n",
    "(mean_abs_jacobian.abs()>0).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.07% (1.3e+08 elements) are above 0e+00\n",
      "41.64% (1.1e+08 elements) are above 1e-06\n",
      "15.35% (4.1e+07 elements) are above 1e-05\n",
      "1.77% (4.8e+06 elements) are above 1e-04\n",
      "0.32% (8.6e+05 elements) are above 1e-03\n",
      "0.02% (5.0e+04 elements) are above 1e-02\n",
      "0.00% (1.4e+01 elements) are above 1e-01\n",
      "0.00% (0.0e+00 elements) are above 1e+00\n"
     ]
    }
   ],
   "source": [
    "for thresh in [0, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]:\n",
    "    above_thresh = mean_abs_jacobian>thresh\n",
    "    print(f\"{100*above_thresh.float().mean().item():.2f}% ({above_thresh.sum().item():.1e} elements) are above {thresh:.0e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vary inputs, see how that changes the output variables where the partial derivative is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(default_prompt, names_filter=[f\"blocks.{layer}.ln2.hook_normalized\"])\n",
    "mlp_in = cache[\"normalized\", layer, \"ln2\"][0, 1:]\n",
    "mlp_in_features = sae.encode(mlp_in, False)\n",
    "mlp_out_features = sandwich(mlp_in_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for mlp_in_feats, mlp_out_feats in tqdm(zip(mlp_in_features, mlp_out_features)):\n",
    "    sliced_jac = get_sliced_jac(mlp_in_feats)\n",
    "    small_indices_in_sliced = (sliced_jac.abs() < 5e-3).nonzero()\n",
    "    \n",
    "    break #!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([443, 2])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_indices_in_sliced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  286,   683,   844,  1170,  1705,  1756,  2013,  3080,  4117,  5289,\n",
       "          5595,  5861,  6225,  8014,  8591,  8647,  9356,  9410,  9931, 10175,\n",
       "         10516, 10732, 10899, 12249, 12618, 13132, 13644, 13981, 14614, 15219,\n",
       "         15537, 16085], device='mps:0'),\n",
       " tensor([  307,   365,   479,   792,   936,  1302,  3338,  3507,  4307,  4366,\n",
       "          4463,  4485,  4560,  5150,  6380,  6394,  6712,  8943,  9023,  9692,\n",
       "          9714,  9717,  9982, 10270, 10608, 10756, 10976, 11524, 11781, 12469,\n",
       "         12590, 12671], device='mps:0'))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mlp_in_feats>0).nonzero().flatten(), (mlp_out_feats>0).nonzero().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
